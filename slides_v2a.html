<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Unraveling principles of motor control: from nerve nets to neural prosthetics</title>
    <meta name="viewport"
	  content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <!--<link rel="stylesheet" href="reveal.js/css/reveal.min.css">-->
    <link rel="stylesheet" href="reveal.js/css/reveal.min.css">
    <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
    <link rel="stylesheet" href="cust_black.css" id="theme">
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
    <script>
      document.write('<link rel="stylesheet" href="reveal.js/css/print/'+(window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper')+'.css" type="text/css" media="print">');
    </script>
    <script src="js/three.js"></script>
    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->

<!-- Printing and PDF exports -->
<script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>
  </head>

  <body>
    <!-- Slides! -->

    <div class="reveal">
      <div class="slides">
        <section data-background="assets/hydra1.jpg">
          <h1>Unraveling principles of motor control: from nerve nets to neural prosthetics</h1>
	  <br><br>
	  <p style="text-align: center; font-size: larger; text-shadow: 0px 0px 6px #000000;">Computational neuroscience seminar<br>Ben Lansdell</p>

	  <aside class="notes">
	    <span style="color: red">
	    </span> •
	    • <span style="color: green"></span>
	  </aside>
        </section>

<!-- ----------------------------------------------------------------------- -->

	<section data-state="neurons2">
		<h2>Outline</h2>
		<ol>
			<li class="fragment highlight-green"> Whole animal imaging in cnidarian Hydra
			<li> Motor encoding in concurrent use brain-computer interfaces
		</ol>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Of course, such comprehensive brain recordings are at present only feasible in
	    smaller and more simple organisms.

	    In most cases, we only observe a vastly undersampled subset of neural activity.
	    Typically from a particular region, of, say cortex. And we learn what we can from the recorded neurons activity and its relation with external variables.
	    • <span style="color: green"></span>
	  </aside>	</section>

	<section>
	  <div style="text-align: left">
		<p>Neural activity encodes features of stimuli and behavior</p>
	      If an individual neuron spikes at times $\{t_i\}$: $$y(t)=\sum_{i=1}^{Y(T)}\delta(t-t_{i}),$$
	      then, in general $$y(t)\sim f[x(t)].$$
	      But, starting simply, $$y(t)\sim f(k*x(t)),$$
	      where $k(\tau)$ are <em>feature vectors</em>.
	  </div>

	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Neural encoding models</h2>
	    <img src="assets/glmuncoupled_white.svg" width="60%">
	    <p class="rcred">Adapted from Aljadeff et al Neuron 2016</p>
	  	<div style="text-align: left;">
	  		<br><br>
	       $$\begin{align*}y_t &\sim \text{Poisson}(\lambda_t),\\
	       \lambda_t &= f[\mathbf{x}_t, y_t] = f\left(\mu + \sum_i k_i x_{t-i} + \alpha_i y_{t-i}\right)\end{align*}$$</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    For instance, a common approach is the following model. Where x is some extrinsic variable. We might expect these models to work best in primary sensory and motor regions of cortex. I would say that models such as these have found most success in sensory systems. In motor cortex, the encoding of motor output to neural activity is less clear. 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Encoding in primary motor cortex</h2>
	    <img src="assets/pref_dir_motor.png">
	    <p class="rcred">Scott et al Nature 2001</p>
	    <ul>
	    	<li> Individual neurons in M1 encode parameters related to kinetics and kinematics
	    	<li> BUT tuning is mixed and labile
	    </ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Multi-unit encoding in primary motor cortex</h2>
	    <div id="left">
	    <ul>
	    	<li>Population vector (PV) encodes reach direction in center-out tasks 
	    	<li>BUT PV can be biased, only applies in certain situations
    	</ul></div>
	    <div id="right">
	    <img src="assets/prefdir.png">
	    <p class="rcred">Georgopoulos et al 1983</p>
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Latent dynamics through multi-unit recordings</h2>
	    <p>Assume neural activity relies also on $\mathbf{z}_t$, internal processes which we do not observe directly: </p>
	    	<div id="dm">
	    	$$\lambda_t^j = f^j[\mathbf{x}_t, \mathbf{z}_t]$$
			</div>
			<p>
	    	<!-- $\Rightarrow$ Recording from large populations of neurons allows inference of $\mathbf{z}_t$<br>  -->
	    	$\Rightarrow$ Estimate $\mathbf{z}_t$ through dimensionality reduction, state-space models, etc</p>
	    <img src="assets/intrinsic_manifold_white.svg" width="55%">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

<!-- ----------------------------------------------------------------------- -->

	<section>
	  <h2>Motor encoding and designing brain-computer interfaces</h2>
	  	<div id="left">
	  	<ul>
	  		<li> Single unit BCIs can be trained through biofeedback
	  		<li> Peak performance independent of PD<br>[Moritz et al 2007]
	  	</ul>
	  	</div>
	  	<div id="right">
	    <img src="assets/law_pd_perf_comb.png">
	    <p class="rcred">Law et al 2014</p>
	  	</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    These questions of motor encoding bear on the design of brain-computer interfaces for the purposes of controlling a neural prosthetic. 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Motor encoding and designing brain-computer interfaces</h2>
	  	<div id="left">
	    <p>Conversely</p>
	  	<ul>
	  		<li> Neural activity during brain-control relies on natural motor reportoire
	  		<li> This poses constraints on learning BCI tasks
	  	</ul>
	  	</div>
	  	<div id="right">
	    <img src="assets/int_manifold.png">
	    <p class="rcred">Sadtler et al 2015</p>
	  	</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Motor encoding and designing brain-computer interfaces</h2>
	  	<div id="left">
	    <p>Conversely</p>
	  	<ul>
	  		<li> Neural activity during brain-control relies on natural motor reportoire
	  		<li> This poses constraints on learning BCI tasks
	  	</ul>
	  	</div>
	  	<div id="right">
	    <img src="assets/int_manifold2.png">
	    <p class="rcred">Sadtler et al 2015</p>
	  	</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Dual-control brain-computer interfaces</h2>
	    <div id="left">
		<br>
	    Applications:
	  	<ul>
	  		<li> Allow stroke patients to regain functionality through co-opting
	  			healthy motor cortex to control BCI in conjunection with residual movement 
	  		<li> Supernumerary BCIs
	  		<li> Insight into learning novel coordination tasks
	  	</ul>
	  	<p class="fragment">$\Rightarrow$ How does motor cortical activity coordinate to accomplish this task?</p>
 	    </div>
 	    <div id="right">
	    <img src="assets/dualcontrolBCI.png">
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Dual-control brain-computer interfaces</h2>
	    <p>
	    Setup:</p>
	  	<ul>
	  		<li> One monkey implanted with multi-electrode Utah array in hand/wrist area of primary motor cortex
	  		<li> Performs random target pursuit task </ul> 
	    <img src="assets/Figure 1_monkey_ver2.png">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Dual-control brain-computer interfaces</h2>
	    <div id="left">
	  		Simple linear tuning model:
	  		$$ n^i_t = \alpha_x x_t + \alpha_y y_t + c + \epsilon_t$$<br>
	  		<ul>
	  		<li>Provides measure of preferred tuning angle $\theta$ and tuning strength<br>
	  		<li>Units chosen with preferred tuning 90 rotated from dual control direction
	  		</ul>
 	    </div>
 	    No relation between tuning strength and performance
 	    <div id="right">
	    <img src="assets/dc_perf.png">
	    <p class="rcred">Milovanovic et al 2015</p>
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	  		<br><br> First result: amazing brain flexibility (Ivana's paper)
	  	Is this flexibility supported by similar activity to that observed in BC?
	  	Look at other recorded units to gain insight 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Changes in tuning of non-control units</h2>
	  	<ul>
	  		<li> Brain-control induces control-unit specific changes in tuning angle
	  		<li> Dual-control does not
	  	</ul>
	    <img src="assets/fig2a.png">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Changes in tuning of non-control units</h2>
	    <img src="assets/fig2b.png" width="50%">
<p>In dual-control tuning angles are more similar to manual control than to brain control</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Changes in functional connectivity of population</h2>
	  <p>
	  	How do relations bewteen units differ in brain- and dual-control tasks?<br><br>

	  	For a fixed network of units, compute transfer entropy between units in each condition:
	  	$$
	  	H_{X\to Y} = I(Y_t|Y_{t-1}, \dots, Y_{t-T}) - I(Y_t|Y_{t-1}, \dots, Y_{t-T},X_{t-1}, \dots, X_{t-T})
	  	$$
	  	for Shannon entropy $I$. <br><br>

	  	Study differences in connectivity between brain-, dual- conditions and manual condition.
	  </p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Changes in functional connectivity of population</h2>
	    <img src="assets/fig3.png" width="50%">
	  	<ul>
	  		<li> Brain-control: overall decrease in functional connectivity to control units
	  		<li> Dual-control: functional connectivity between co-tuned units does not change, except when control unit involved
	  	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Alignment of 'brain-control space' and 'intrinsic variability' predicts performance</h2>
	  	<ul>
	  		<li> Neither tuning nor functional connectivity analysis predict performance
	  		<li> Examine co-activity patterns during manual control -- 'intrinsic variability'
	  		<li> Measure with GPFA
	  	</ul>
	    <img src="assets/intrinsic_manifold_white.png" width="70%">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Alignment of 'brain-control space' and 'intrinsic variability' predicts performance</h2>
	  	<ul>
	  		<li> Neither tuning nor functional connectivity analysis predict performance
	  		<li> Examine co-activity patterns during manual control -- 'intrinsic variability'
	  		<li> Measure with GPFA
	  	</ul>
	    <img src="assets/gpfa_perf.png" width="30%">

	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Cursor control requires intrinsic variance of control units</h2>
	  <div id="left">
	  	<p>
	  		BCI cursor control:
	  		$$
	  		x_t = \sum_{n=1}^N \alpha_n(y_t^n - \beta_t^n)
	  		$$
	  		<ul>
	  		<li>$\beta_t^n$ moving estimate of baseline firing of unit $n$.
	  		<li>Linear, moving average model <br>$\Rightarrow$ use Granger causality to quantify unit contributions to cursor control, $\mathcal{G}$</ul>
	  	</p>
	  </div>
	  <div id="right">
	    <img src="assets/Figure5_ver2a.png">
	</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Cursor control requires intrinsic variance of control units</h2>
	    <img src="assets/Figure5_ver2.png">
	  	<ul>
	  		<li> Intrinsic variance of units predicts cursor contribution in dual-control task
	  		<li> High dual-control performance only occurs when at least one control unit has high intrinsic variability
	  	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Summary</h2>
	  	<ul>
	  		<li> Tuning and connectivity analysis suggest dual-control task generates cortical activity more similar to that observed in manual control task
	  		<li> Intrinsic variance of control units only variable found to predict performance and control unit contributions -- motor unit tuning does not constrain how the task is performed
	  	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>


	<section data-state="neurons" data-transition="fade-in">
		<h2>Outline</h2>
		<ol>
			<li class="fragment highlight-green"> Whole animal imaging in cnidarian Hydra
			<li> Motor encoding in concurrent use brain-computer interfaces
		</ol>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Totally generically, an active question we all pursue is how do we determine a neural circuit's
	    function in coordinating its behavior?

	    In cortex, for example, we might look at encoding of stimuli or motor output
	    and infer something about function from these relationships. However, applying
	    this idea directly to, say, primary motor cortex of primates leads does not
	    provide clear indication of the function of M1 -- the nature of encoding
	    in M1 remains unclear.

	    Thus other frameworks may be needed to better uncover the role of M1. Testing
	    these would benefit from having more comprehensive measures of neural activity.
	    Thus there is obvious value in developing technologies that can record from large
	    or comprehensive measures of neural activity in conjunction with its behavior, as has been performed in Zebrafish and C elegans.

	    With this in mind, one place to start is with is with very simple organisms with simple nervous systems.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-background-color="#000000">
	<h2>Whole-animal imaging in unconstrained Hydra</h2>
	  <div>
	<video src="assets/hydra_feeding_short.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="70%"></video>
	 <div class="fragment">
	<p>Why?</p>
	 <ol>
	 	<li> Small (0.5mm -- 1.5cm) -- can fit into FOV of traditional microscope
	 	<li> Translucent; nerve net, easier imaging
	 	<li> Does not age, and can regenerate
	 </ol>
	</div>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    The Hydra has a diffuse nerve net, one of the simplest nervous systems.

	    As part of a collaboration with Yuste lab at Columbia, among others, we
	    are working on obtaining this level of knowledge in the cnidarian Hydra.

	    For this, the Hydra has the advantages that:
	    -it is transparent, making imaging simpler
	    -it is small, allowing it to fit into the FOV of a traditional scope
	    -it consists of a nerve net, meaning that neurons are spread throughout
	    the organism and can be imaged more or less without obstructing one another
	    -it can be constrained to lie in a 2D plane, meaning more or less the organism
	    can be in focus all at once, without the need for z-slicing, which permits a high
	    temporal resolution recording.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Hydra anatomy</h2>
	  <div>
	    <img src="assets/hydra_layers_white.png" width="100%" position="left">
	    <p class="rcred">Adapted from Technau and Steele 2011</p>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Describe Hydra anatomy

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Hydra behavior</h2>
	  <div>
	    <img src="assets/hydra_behavior.png" width="60%" position="left">
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Aims</h2>
<hr>Understand (and control) neuronal basis of simple behavior such as contracting and expelling
	    water.</p>
	     <div>
<hr><p>Sub-aims:</p>
	  	<ol>
	  	<li class="fragment highlight-green">Track Hydra pose</li>
	  	<li>Register and track neurons</li>
	  	<li>Record neural activity</li>
	  </ol>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • Aims: understand (and control) simple behavior such as contracting and expelling
	    water.

	    Hydra periodically absorbs water (possibly through osmosis) and expells it all at once
	    once reaching a certain size. Nerve free Hydra do not expell fluid, so we know this is
	    a nervous system function.	

	    Sub aim:
	    In achieving this, a sub-aim is to be able to track the Hydra's pose and neurons throughout
	    unconstrained behavior. This allows analysis of the Hydra's behavior, it's neural activity,
	    and of course the relationship between the two.

	    I will spend the 1st half of this talk describing a method for performing this tracking in
	    videos of Hydra in which neurons have been tagged with GCamp6, allowing for their neural
	    activity to be imaged. 

	    Mention this is a collaboration with Rafa Yuste lab in Columbia
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Experiment</h2>
	  <div id="left">
	    <img src="assets/hydrasetup.jpg" width="50%">
	    <p class="rcred"></p>
	  </div>
	  <div id="right">
	    <img src="assets/hydrasetup3.jpg" width="100%" position="left">
	    <p class="rcred"></p>
	  </div>
	    <p>Methods</p>
	    <ul>
	    <li> Create Act-GCaMP6s transgenic Hydra
		<li> Mount between coverslips separated by .1mm spacer
		<li> Image calcium transients
		</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Methods
	    - Hydra details
	    Genetic calcium indicator. Modified plasmid which expresses GFP, to express GCaMP6.
	    Injected in Hydra fertilized eggs. (Act-GCaMP6s transgenic Hydra)
	    - imaging methods
	    mounted specimens between two coverslips separated by .1mm spacer. Hollow cylinder
	    imaged from the side. Transparent means can image activity of every one of its neurons.
	    - experiment details:
	    transients digital -- all or none -- fast risetimes and slow decays. Hydra neurons have
	    action potentials, express sodium channels in their genome, simplest explanation
	    of changes in flouresence is due to calcium influx due to action potential activity.
	    As in mammalian neurons. Did not record Ca signals of intermediate amplitude suggests that
	    capturing individual action potentials.

	    • <span style="color: green"></span>
	  </aside>
	</section>

<!-- ----------------------------------------------------------------------- -->

	<section>
	<h2>Whole-body calcium imaging in Hydra</h2>
	<video src="assets/hydra_gcamp.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="75%"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    How do we go about doing this tracking?

	    Some issues:
	    - neuron tracking (which if reliable would provide information about neural
	    activity and body position) is problematic since not all neurons are visible
	    at all times.
	    - particle tracking thus has a track association problem
	    - how are neurons reassociated once they become inactive then active again?

	    As an aside, note that in the future we may have access to labeled neurons
	    through the use of nanoparticles or genetic labels. This would greatly aid in the
	    neuron tracking and body tracking by provided clear fiducial markers that can be consistently
	    tracked. However, this is not available yet and remains an experimental
	    challenge. Further, not all Hydra strains may have such labeling, and we would like
	    methods that can tracking Hydra body in cases when we have flourescent labels for
	    activity in epithelial cells which show muscle activity. In these datasets, individual
	    cells may be hard to track and make out. Since we would like to do behavior analysis on
	    these dataseta also, we will here tackle the problem of performing body tracking
	    in cases where explicit fiducial markers are not present.

	    For today let's take this as our problem.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-background-color="#000000">
	<h2>Deformable object tracking</h2>
	<div id="left">
	<video src="assets/kalmanfilter_slow.mp4" muted controls
	 poster="assets/dst2.png" class="slideautostart"></video>
	</div>
	<div id="right">
		<p>Extended Kalman filter tracking

			$$\begin{align}y_t&=h(x_t) +\nu_t \\
			x_t &= f(x_{t-1}) +\epsilon_t\end{align} $$ 

			Where $x_t$ is the Hydra positions and velocities, $y_t$ is the generated
			image.<br> <br>
		</p>
		<ul>
			<li>High-dimensional -- slow<br>
			<li>Unstable -- once tracking lost, difficult to recover
		</ul>

	</div>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    How do we go about doing this tracking?

	    A solution is to then fit a parameterization or mesh to the body explicitly
	    and to use this to perform neuron tracking.

	    We can do this by adapting methods from deformable object tracking.

	    However, as a very high dimensional state space model. these methods are slow, unstable, and difficult to recover from difficult Hydra motion. Once it's lost the Hydra, it can't recover.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-background-color="#000000">
	<h2>Multi-frame optic flow image registration</h2>
	<video src="assets/mfsf_demo.mp4" muted controls
	 poster="assets/dst2.png" class="slideautostart"></video>
	 <p class="rcred">Ravi Garg, Anastasios Roussos, Lourdes Agapito, International journal of computer vision 104 (3), 286-314, 2013</p>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    Instead, we can use a multi-frame optic flow method, which determines
	    deformations of a template image that balances smoothness with image
	    correspondence to later frames through an energy minimization approach. Faster and more stable. 

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Multi-frame optic flow image registration</h2>
	<img src="assets/mfsf_white.svg">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Multi-frame optic flow image registration</h2>
	<ul id="blanklist">
	<li> Dense optic flow with subspace constraints
	<li> For each pixel find linear combination of basis paths, $L$, that minimize energy<br>
	<img src="assets/energyeq.png">
	<li> $I_f$ = image at frame $f$,
	<li> $I_0$ = reference frame (need not be first frame of video),
	<li> $Q_f^u$, $Q_f^v$ = basis paths at frame $f$, 
	<li> $\alpha$ = smoothness regularizer
	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Tracking with optic flow</h2>
	<video src="assets/warp_neurons_nref100.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart"></video>
	 <ul> 
	 <li> blue = hand tracked neurons
	 <li> green = w/in 6px of 'true', red = >6px of 'true'
	 </ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	   	We can see that this work fairly well for short sequences of video.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Tracking with optic flow: performance</h2>
		<div id="left">
		<p>Comparison to hand annotated neuron tracks<br>
		Per frame:</p>
		<ul>
		<li> at least 52% neurons are tracked within 6px
		<li> on average 82% neurons tracked within 6px
		</ul>
		Per neuron:
		<ul>
		<li> 42% neurons tracked within 6 px throughout all video
		</ul>
		</div>
		<div id="right">
		<img src="assets/tracking_errors.png">
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    This method works fairly well for short sequences of video. Tracking 42%
	    of all neurons is high enough to start to be useful, even if there's plenty
	    of room for improvement.

	    Further, it is fast and stable.

	    However, it is not always appropriate to use the one reference frame for the
	    Hydra, since it changes position quite significantly throughout a recording.
	    (show video)
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Extending to longer sequences</h2>
		<img src="assets/mfsf_stitch_white.svg" width="70%">
		<div class="fragment">
		How to mitigate accumulation of errors from video to video?
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
		Changing appearance/pose in a long sequence → multiple reference frames
		Simplest approach: split into multiple stacks and run independently
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Choosing which paths to extend</h2>
	Measure image registration error
		<img src="assets/combined.png">
		<div>
		<div class="column-left">Forward map $$g_{1,2}(\mathbf{x})$$</div>
		<div class="column-center">Reverse map $$g_{2,1}(\mathbf{x})$$</div>
		<div class="column-right">Error $$f_{1,2}(\mathbf{x}) = \left|g_{2,1}(g_{1,2}(\mathbf{x}))\right|$$</div>
		</div>
		<br><img src="assets/colorwheel.png" align="left">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>


	<section>
	<h2>Exploiting periodicity in Hydra behavior</h2>
	<div id="left">
	<ul> 
	<br><br><br>
	Stereotyped Hydra behavior:<br>elongate then contract<br><br>
	Measure optic flow error, $f_{ij}(x)$, between frames with regular spacing
	<br><br>'iframes' -- every 250 frames
	</ul>
	</div>
	<div id="right" style="text-align: center;">
		$\langle f_{ij}(x)\rangle_\Omega$
		<img src="assets/similarity_orig.png">
		Frame index
	</div>

	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Exploiting periodicity in Hydra behavior</h2>
	<div id="left">
	<p> 
	Two clusters: contracted and elongated<br>
	$\Rightarrow$ By registering regions of each iframe with a ref. frame we extend paths into temporally distant but positionally related frames<br>
	$\Rightarrow$ A mechanism to handle arbitrarily long videos without accumulation of tracking error
	</p>
	</div>
	<div id="right" style="text-align: center;">
		$\langle f_{ij}(x)\rangle_\Omega$<br>
		<img src="assets/dendrogram.png" width="70%">
	</div>
		<img src="assets/dend_d1_tile_c1.png" width="80%">
		<img src="assets/dend_d1_tile_c2.png" width="80%">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Segmenting regions to extend</h2>
	<div id="left">
		Mumford-Shah image segmention: divide into $k$ regions of constant color, while trying to minimize perimeter of region boundaries
	</div>
	<div id="right"><img src="assets/butterfly_together.png"></div>
		$$\begin{align*}\min_{(R_l),c_l}\frac{1}{2}\sum_{l=1}^k \text{Per}(R_l;\Omega)
			+ \frac{\lambda}{2}\sum_{l=1}^k \int_{R_l} |g(x)-c_l|^2\,dx\end{align*}$$
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Segmenting regions to extend</h2>
	<div id="left">
		Mumford-Shah image segmention: divide into $k$ regions of constant color, while trying to minimize perimeter of region boundaries
	</div>
	<div id="right"><img src="assets/butterfly_together.png"></div>
	<p>
		$$\begin{align*}\min_{u_l}\frac{1}{2}\sum_{l=1}^k \int_\Omega |\nabla u_l|\,dx
			+ \frac{\lambda}{2}\sum_{l=1}^k \int_{\Omega} u_l(x)f_l(x)\,dx\end{align*}
		$$
		with $f_l(x) = |g(x)-c_l|^2$. Assume $c_l$ are known and $\mathbf{u}\in U$:
		$$
		U = \left\{u_l:\sum_l^k u_l(x) = 1, \quad u_l(x) \ge 0, \forall x\in\Omega\right\}
		$$
		Convex in $\mathbf{u}$<br>
		Select color via $v(x) = \text{argmax}_l u_l(x)$ 
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Mumford-Shah image segmentation</h2>
	<p>
		Chambolle algorithm solves problems:
		$$\min_{x\in X}F(Kx) + G(x)$$
		for convex $F(\cdot):Y\to [0,\infty]$ and $G(\cdot):X\to [0,\infty]$, 
		<br>in primal-dual form:
		$$\min_{x\in X}\max_{y\in Y} \langle Kx, y\rangle - F^*(y) + G(x)$$
		<em>Algorithm:</em>
	</p>
	<ol>
	<li> Initialization: $\tau, \sigma > 0, \theta \in [0,1], (x^0,y^0)\in X\times Y$. Set $\bar{x}^0 = x^0$
	<li> Iterate until convergence: ($n\ge 0$)
		$$\begin{align}
		y^{n+1} &= \pi_{F^*}(y^n + \sigma K \bar{x}^n; \sigma)\\
		x^{n+1} &= \pi_G(x^n - \tau K^* {y}^{n+1}; \tau)\\
		\bar{x}^{n+1} &= x^{n+1} +\theta(x^{n+1} - x^n)
		\end{align}$$
	</ol>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Mumford-Shah image segmentation</h2>
	<p>
		With proximal operator
		$$
		\pi_G(y;\tau) = \text{argmin}_{x}\frac{\|x-y\|_2^2}{2\tau}+G(x)
		$$
		<br>
		Primal-dual MS image segmentation:  
		$$\begin{align}\min_{u=(u_l)_{l=1}^k} \max_{p=(p_l)_{l=1}^k} &\left(\sum_{l=1}^k\langle \nabla u_l, p_l \rangle +\langle u_l, f_l \rangle \right) +\delta_U(u) - \delta_P(p) \end{align}$$
		So,
		<ul>
		<li> $K = \nabla$ (first order forward difference)
		<li> $K^* = -\text{div}$ (first order backward difference)
		<li> $F^*(p) = \delta_P(p)$ with $P=\left\{ p\in Y^k:\|p_l\|_\infty \le \frac{1}{2}\right\}$
		<li> $G(u) = \delta_U(u)$
		</ul>
		Proximal operaters $\pi_{F^*}$ and $\pi_G$ are pixel-wise projection onto convex sets $P$ and $U$
		$\Rightarrow$ Easy to implement on GPU
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<!-- <section>
	<h2>Mumford-Shah image segmentation</h2>
	<div id="left">
		Proximal operaters $\pi_{F^*}$ and $\pi_G$ are pixel-wise projection onto convex sets $P$ and $U$<br>
		$\Rightarrow$ Easy to implement on GPU
	</div>
	<div id="right"></div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section> -->

	<section>
	<h2>Segmentation of label tracked regions</h2>

		<p>For $K$ reference frames and $L$ iframes, let $f_{ij}(x)$ represent the optic flow error in using reference image $i$ to construct image $j$.  
		$$\begin{align}\min_{u} \max_{p} \sum_{l=1}^L\left(\sum_{k=1}^K\langle \nabla u_{kl}, p_{kl} \rangle +\langle u_{kl}, f_{kl} \rangle \right) +\delta_U(u) - \delta_P(p) \end{align}$$
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			1. Select very sparse set of reference frames (ref frames)<br>
			2. Select regular set of inter-frames (iframes)<br>
			<img src="assets/dend_d1_tile_c1_hl.png" width="90%"><br>
			<img src="assets/dend_d1_tile_c2_hl.png" width="90%"><br>
			3. Use optic flow+image segmentation to label regions mapping to reference frames<br>
			<img src="assets/mfsf_extend1.png" width="90%"><br>
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			4. Within each iframe block run MFSF for dense registration<br>
			<img src="assets/mfsf_extend2.png" width="90%"><br>
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			5. Associate each path from (4) with a ref frame using segmentation (3)<br>
			<img src="assets/mfsf_extend3.png" width="90%"><br>
		</p>
		<p class="fragment">
			Thus every tracked path is associated with a point in a reference frame
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
		Thus every tracked path is associated with a point in a reference frame
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Choosing ref frames</h2>
	<p>
		Want number of ref frames to balance global registration vs registration error <br><br>
		Add a group LASSO penalty for number of reference frames used:
		$$\begin{align}\min_{u} \sum_{l=1}^L\left(\sum_{k=1}^K \langle u_{kl}, f_{kl} \rangle \right) +\delta_U(u) - \delta_P(p) + \frac{\lambda_2}{2}\sum_{k=1}^K\left(\sum_{l=1}^L \|u_{kl}\|^2_2 \right)^{1/2} \end{align}$$
		The proximal operator $\pi_G$ now becomes:
		$$
		\pi_G(y;\tau) = \text{argmin}_{x}\frac{\|x-y\|_2^2}{2\tau}+\frac{\lambda_2}{2}\sum_{k=1}^K\left(\sum_{l=1}^L \|y_{kl}\|^2_2 \right)^{1/2}+\delta_U(y)
		$$
		Compute $\pi_G$ with ADMM


	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Extending with $K=2; L = 8$</h2>
	<video src="assets/mfsf_dm_combined.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="55%"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •
	   	We can see that this work fairly well for short sequences of video.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<!-- <section>
	<h2>Combining reference frames through temporal continuity of neuron tracking</h2>
	<video src="assets/mfsf_dm_combined.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •
	Allows for registration of neurons over videos of indefinite length
	    • <span style="color: green"></span>
	  </aside>
	</section> -->

	<section>
	<h2>Future work</h2>
	<ul>
		<li> Create larger hand annotated datasets for performance evaluation
		<li> Use temporal continuity between adjacent blocks to relate reference frames to one another
	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    This approach has utility as a general optic flow technique that can handle occlusions -- an important problem in computer vision.

	    We've seen how this method can be used to track Hydra, which we believe will
	    form an important component of determining the neural basis of its behavior.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<!--<section>
	  <h2>WebGL makes toroids</h2>
	  <div class="ctr w70">
		<canvas data-sample="spinning_wireframe_torus"></canvas>
	  </div>

	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>-->

	<section data-state="neurons" data-transition="fade-in">
	  <aside class="notes">
	    <span style="color: red"></span> •
Returning to our theme of recording from large populations of neurons. I think the fact that we don't observe
insights into how the task is performed, or constraints on task performance based on tuning angles, and the fact that we do gain some insight from methods related to internal dynamics and dimensionality reduction suggests that the way forward to understanding motor cortical activity, not surprisingly is through high-dimensional recordings and better characterizing internal dynamics.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Acknowledgments</h2>
	  <div id="left">
	  	<ul>
         <li> Adrienne Fairhall</li>
         <li> Chet Moritz</li>
         <li> Ivana Milovanovic</li>
         <li> Cooper Mellema</li>
         <li> Eberhard Fetz</li>
	  	</ul>
   	  </div>
	  <div id="left">
	  	<ul>
         <li> Fairhall lab</li><ul>
         	<li> Anatoly Buchin </li></ul>
         <li> Moritz lab</li><ul>
         	<li> Charlie Matlack </li>
         	<li> Robert Robinson</li></ul>
         <li> Yuste lab</li><ul>
         <li> Christophe Dupre</li>
         <li> John Szymanski</li></ul>
	  	</ul>
   	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

    <!--<script src="reveal.js/js/reveal.min.js"></script>-->
    <script src="reveal.js/js/reveal.js"></script>
    <script src="pdfjs/compatibility.js"></script>
    <script src="pdfjs/pdf.js"></script>

    <script src="lib/js/head.min.js"></script>
    <script>
      head.js(
        "lib/js/jquery.min.js",
        "lib/js/jquery.hotkeys.js",
        "lib/js/underscore.min.js",
        "lib/js/swfobject.js",
        "lib/js/dat.gui.js",
        "lib/js/EventEmitter.js",

        "lib/js/three.js",
        "lib/js/three/EffectComposer.js",
        "lib/js/three/RenderPass.js",
        "lib/js/three/BloomPass.js",
        "lib/js/three/ShaderPass.js",
        "lib/js/three/MaskPass.js",

        // three shaders
        "lib/js/three/shaders/CopyShader.js",
        "lib/js/three/shaders/BasicShader.js",
        "lib/js/three/shaders/DotScreenShader.js",
        "lib/js/three/shaders/UnpackDepthRGBAShader.js",
        "lib/js/three/shaders/HorizontalBlurShader.js",
        "lib/js/three/shaders/VerticalBlurShader.js",

        // js files needed for WebGL specific samples (excluding three js)
        "lib/js/J3DI.js",
        "lib/js/J3DIMath.js",
        "lib/js/webgl-utils.js",
        "lib/js/webgl-debug.js",

        // App specific js
        //"js/reveal.min.js",
        "js/stats_bootstrap.js",
        "js/samples.js",
        //"js/dat.gui.bootstrap.js",

		function() {
      Reveal.initialize({
        controls: false,
        progress: true,
        history: true,
        center: false,
        keyboard: true,
        touch: false,
        overview: true,
        mouseWheel: false,
        width: 960,
        height: 720,

        theme: false, // hardcoded with CSS import in <head>
        transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none
        transitionSpeed: 'default', // default/fast/slow

        math: {
          mathjax: 'mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        dependencies: [
          { src: 'reveal.js/lib/js/classList.js',
	    condition: function() { return !document.body.classList; }},
          { src: 'reveal.js/plugin/markdown/marked.js',
	    condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal.js/plugin/markdown/markdown.js',
	    condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true,
	    callback: function() { hljs.initHighlightingOnLoad (); }},
          { src: 'reveal.js/plugin/notes/notes.js', async: true,
	    condition: function() { return !!document.body.classList; }},
          { src: 'mymath.js', async: true },
	  { src: 'pdfimgs.js', async: true },
	  { src: 'slideautostart.js', async: true },
        ],
      });
    });
    </script>
  </body>
</html>
