<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Unraveling principles of motor control: from nerve nets to neural prosthetics</title>
    <meta name="viewport"
	  content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <!--<link rel="stylesheet" href="reveal.js/css/reveal.min.css">-->
    <link rel="stylesheet" href="reveal.js/css/reveal.min.css">
    <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
    <link rel="stylesheet" href="cust_black.css" id="theme">
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
    <script>
      document.write('<link rel="stylesheet" href="reveal.js/css/print/'+(window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper')+'.css" type="text/css" media="print">');
    </script>
    <script src="js/three.js"></script>
    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->

<!-- Printing and PDF exports -->
<script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>
  </head>

  <body>
    <!-- Slides! -->

	<section data-background-color="#000000">
	<h2>Whole-animal imaging in unconstrained Hydra</h2>
	  <div>
	<video src="assets/hydra_feeding_short.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="70%"></video>
	 <div class="fragment">
	<p>Why?</p>
	 <ol>
	 	<li> Small (0.5mm -- 1.5cm) -- can fit into FOV of traditional microscope
	 	<li> Translucent; nerve net, easier imaging
	 	<li> Does not age, and can regenerate
	 </ol>
	</div>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    The Hydra has a diffuse nerve net, one of the simplest nervous systems.

	    As part of a collaboration with Yuste lab at Columbia, among others, we
	    are working on obtaining this level of knowledge in the cnidarian Hydra.

	    For this, the Hydra has the advantages that:
	    -it is transparent, making imaging simpler
	    -it is small, allowing it to fit into the FOV of a traditional scope
	    -it consists of a nerve net, meaning that neurons are spread throughout
	    the organism and can be imaged more or less without obstructing one another
	    -it can be constrained to lie in a 2D plane, meaning more or less the organism
	    can be in focus all at once, without the need for z-slicing, which permits a high
	    temporal resolution recording.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Hydra anatomy</h2>
	  <div>
	    <img src="assets/hydra_layers_white.png" width="100%" position="left">
	    <p class="rcred">Adapted from Technau and Steele 2011</p>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Describe Hydra anatomy

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Aims</h2>
<hr>Understand (and control) neuronal basis of simple behavior such as contracting and expelling
	    water.</p>
	     <div>
<hr><p>Sub-aims:</p>
	  	<ol>
	  	<li class="fragment highlight-green">Track Hydra pose</li>
	  	<li>Register and track neurons</li>
	  	<li>Record neural activity</li>
	  </ol>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • Aims: understand (and control) simple behavior such as contracting and expelling
	    water.

	    Hydra periodically absorbs water (possibly through osmosis) and expells it all at once
	    once reaching a certain size. Nerve free Hydra do not expell fluid, so we know this is
	    a nervous system function.	

	    Sub aim:
	    In achieving this, a sub-aim is to be able to track the Hydra's pose and neurons throughout
	    unconstrained behavior. This allows analysis of the Hydra's behavior, it's neural activity,
	    and of course the relationship between the two.

	    I will spend the 1st half of this talk describing a method for performing this tracking in
	    videos of Hydra in which neurons have been tagged with GCamp6, allowing for their neural
	    activity to be imaged. 

	    Mention this is a collaboration with Rafa Yuste lab in Columbia
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Experiment</h2>
	  <div id="left">
	    <img src="assets/hydrasetup.jpg" width="50%">
	    <p class="rcred"></p>
	  </div>
	  <div id="right">
	    <img src="assets/hydrasetup3.jpg" width="100%" position="left">
	    <p class="rcred"></p>
	  </div>
	    <p>Methods</p>
	    <ul>
	    <li> Create Act-GCaMP6s transgenic Hydra
		<li> Mount between coverslips separated by .1mm spacer
		<li> Image calcium transients
		</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Methods
	    - Hydra details
	    Genetic calcium indicator. Modified plasmid which expresses GFP, to express GCaMP6.
	    Injected in Hydra fertilized eggs. (Act-GCaMP6s transgenic Hydra)
	    - imaging methods
	    mounted specimens between two coverslips separated by .1mm spacer. Hollow cylinder
	    imaged from the side. Transparent means can image activity of every one of its neurons.
	    - experiment details:
	    transients digital -- all or none -- fast risetimes and slow decays. Hydra neurons have
	    action potentials, express sodium channels in their genome, simplest explanation
	    of changes in flouresence is due to calcium influx due to action potential activity.
	    As in mammalian neurons. Did not record Ca signals of intermediate amplitude suggests that
	    capturing individual action potentials.

	    • <span style="color: green"></span>
	  </aside>
	</section>

<!-- ----------------------------------------------------------------------- -->

	<section>
	<h2>Whole-body calcium imaging in Hydra</h2>
	<video src="assets/hydra_gcamp.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="75%"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    How do we go about doing this tracking?

	    Some issues:
	    - neuron tracking (which if reliable would provide information about neural
	    activity and body position) is problematic since not all neurons are visible
	    at all times.
	    - particle tracking thus has a track association problem
	    - how are neurons reassociated once they become inactive then active again?

	    As an aside, note that in the future we may have access to labeled neurons
	    through the use of nanoparticles or genetic labels. This would greatly aid in the
	    neuron tracking and body tracking by provided clear fiducial markers that can be consistently
	    tracked. However, this is not available yet and remains an experimental
	    challenge. Further, not all Hydra strains may have such labeling, and we would like
	    methods that can tracking Hydra body in cases when we have flourescent labels for
	    activity in epithelial cells which show muscle activity. In these datasets, individual
	    cells may be hard to track and make out. Since we would like to do behavior analysis on
	    these dataseta also, we will here tackle the problem of performing body tracking
	    in cases where explicit fiducial markers are not present.

	    For today let's take this as our problem.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-background-color="#000000">
	<h2>Deformable object tracking</h2>
	<div id="left">
	<video src="assets/kalmanfilter_slow.mp4" muted controls
	 poster="assets/dst2.png" class="slideautostart"></video>
	</div>
	<div id="right">
		<p>Extended Kalman filter tracking

			$$\begin{align}y_t&=h(x_t) +\nu_t \\
			x_t &= f(x_{t-1}) +\epsilon_t\end{align} $$ 

			Where $x_t$ is the Hydra positions and velocities, $y_t$ is the generated
			image.<br> <br>
		</p>
		<ul>
			<li>High-dimensional -- slow<br>
			<li>Unstable -- once tracking lost, difficult to recover
		</ul>

	</div>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    How do we go about doing this tracking?

	    A solution is to then fit a parameterization or mesh to the body explicitly
	    and to use this to perform neuron tracking.

	    We can do this by adapting methods from deformable object tracking.

	    However, as a very high dimensional state space model. these methods are slow, unstable, and difficult to recover from difficult Hydra motion. Once it's lost the Hydra, it can't recover.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-background-color="#000000">
	<h2>Multi-frame optic flow image registration</h2>
	<video src="assets/mfsf_demo.mp4" muted controls
	 poster="assets/dst2.png" class="slideautostart"></video>
	 <p class="rcred">Ravi Garg, Anastasios Roussos, Lourdes Agapito, International journal of computer vision 104 (3), 286-314, 2013</p>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    Instead, we can use a multi-frame optic flow method, which determines
	    deformations of a template image that balances smoothness with image
	    correspondence to later frames through an energy minimization approach. Faster and more stable. 

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Multi-frame optic flow image registration</h2>
	<img src="assets/mfsf_white.svg">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Multi-frame optic flow image registration</h2>
	<ul id="blanklist">
	<li> Dense optic flow with subspace constraints
	<li> For each pixel find linear combination of basis paths, $L$, that minimize energy<br>
	<img src="assets/energyeq.png">
	<li> $I_f$ = image at frame $f$,
	<li> $I_0$ = reference frame (need not be first frame of video),
	<li> $Q_f^u$, $Q_f^v$ = basis paths at frame $f$, 
	<li> $\alpha$ = smoothness regularizer
	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Tracking with optic flow</h2>
	<video src="assets/warp_neurons_nref100.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart"></video>
	 <ul> 
	 <li> blue = hand tracked neurons
	 <li> green = w/in 6px of 'true', red = >6px of 'true'
	 </ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	   	We can see that this work fairly well for short sequences of video.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Tracking with optic flow: performance</h2>
		<div id="left">
		<p>Comparison to hand annotated neuron tracks<br>
		Per frame:</p>
		<ul>
		<li> at least 52% neurons are tracked within 6px
		<li> on average 82% neurons tracked within 6px
		</ul>
		Per neuron:
		<ul>
		<li> 42% neurons tracked within 6 px throughout all video
		</ul>
		</div>
		<div id="right">
		<img src="assets/tracking_errors.png">
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    This method works fairly well for short sequences of video. Tracking 42%
	    of all neurons is high enough to start to be useful, even if there's plenty
	    of room for improvement.

	    Further, it is fast and stable.

	    However, it is not always appropriate to use the one reference frame for the
	    Hydra, since it changes position quite significantly throughout a recording.
	    (show video)
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Extending to longer sequences</h2>
		<img src="assets/mfsf_stitch_white.svg" width="70%">
		<div class="fragment">
		How to mitigate accumulation of errors from video to video?
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
		Changing appearance/pose in a long sequence → multiple reference frames
		Simplest approach: split into multiple stacks and run independently
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Choosing which paths to extend</h2>
	Measure image registration error
		<img src="assets/combined.png">
		<div>
		<div class="column-left">Forward map $$g_{1,2}(\mathbf{x})$$</div>
		<div class="column-center">Reverse map $$g_{2,1}(\mathbf{x})$$</div>
		<div class="column-right">Error $$f_{1,2}(\mathbf{x}) = \left|g_{2,1}(g_{1,2}(\mathbf{x}))\right|$$</div>
		</div>
		<br><img src="assets/colorwheel.png" align="left">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>


	<section>
	<h2>Exploiting periodicity in Hydra behavior</h2>
	<div id="left">
	<ul> 
	<br><br><br>
	Stereotyped Hydra behavior:<br>elongate then contract<br><br>
	Measure optic flow error, $f_{ij}(x)$, between frames with regular spacing
	<br><br>'iframes' -- every 250 frames
	</ul>
	</div>
	<div id="right" style="text-align: center;">
		$\langle f_{ij}(x)\rangle_\Omega$
		<img src="assets/similarity_orig.png">
		Frame index
	</div>

	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Exploiting periodicity in Hydra behavior</h2>
	<div id="left">
	<p> 
	Two clusters: contracted and elongated<br>
	$\Rightarrow$ By registering regions of each iframe with a ref. frame we extend paths into temporally distant but positionally related frames<br>
	$\Rightarrow$ A mechanism to handle arbitrarily long videos without accumulation of tracking error
	</p>
	</div>
	<div id="right" style="text-align: center;">
		$\langle f_{ij}(x)\rangle_\Omega$<br>
		<img src="assets/dendrogram.png" width="70%">
	</div>
		<img src="assets/dend_d1_tile_c1.png" width="80%">
		<img src="assets/dend_d1_tile_c2.png" width="80%">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Segmenting regions to extend</h2>
	<div id="left">
		Mumford-Shah image segmention: divide into $k$ regions of constant color, while trying to minimize perimeter of region boundaries
	</div>
	<div id="right"><img src="assets/butterfly_together.png"></div>
		$$\begin{align*}\min_{(R_l),c_l}\frac{1}{2}\sum_{l=1}^k \text{Per}(R_l;\Omega)
			+ \frac{\lambda}{2}\sum_{l=1}^k \int_{R_l} |g(x)-c_l|^2\,dx\end{align*}$$
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Segmenting regions to extend</h2>
	<div id="left">
		Mumford-Shah image segmention: divide into $k$ regions of constant color, while trying to minimize perimeter of region boundaries
	</div>
	<div id="right"><img src="assets/butterfly_together.png"></div>
	<p>
		$$\begin{align*}\min_{u_l}\frac{1}{2}\sum_{l=1}^k \int_\Omega |\nabla u_l|\,dx
			+ \frac{\lambda}{2}\sum_{l=1}^k \int_{\Omega} u_l(x)f_l(x)\,dx\end{align*}
		$$
		with $f_l(x) = |g(x)-c_l|^2$. Assume $c_l$ are known and $\mathbf{u}\in U$:
		$$
		U = \left\{u_l:\sum_l^k u_l(x) = 1, \quad u_l(x) \ge 0, \forall x\in\Omega\right\}
		$$
		Convex in $\mathbf{u}$<br>
		Select color via $v(x) = \text{argmax}_l u_l(x)$ 
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Mumford-Shah image segmentation</h2>
	<p>
		Chambolle algorithm solves problems:
		$$\min_{x\in X}F(Kx) + G(x)$$
		for convex $F(\cdot):Y\to [0,\infty]$ and $G(\cdot):X\to [0,\infty]$, 
		<br>in primal-dual form:
		$$\min_{x\in X}\max_{y\in Y} \langle Kx, y\rangle - F^*(y) + G(x)$$
		<em>Algorithm:</em>
	</p>
	<ol>
	<li> Initialization: $\tau, \sigma > 0, \theta \in [0,1], (x^0,y^0)\in X\times Y$. Set $\bar{x}^0 = x^0$
	<li> Iterate until convergence: ($n\ge 0$)
		$$\begin{align}
		y^{n+1} &= \pi_{F^*}(y^n + \sigma K \bar{x}^n; \sigma)\\
		x^{n+1} &= \pi_G(x^n - \tau K^* {y}^{n+1}; \tau)\\
		\bar{x}^{n+1} &= x^{n+1} +\theta(x^{n+1} - x^n)
		\end{align}$$
	</ol>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Mumford-Shah image segmentation</h2>
	<p>
		With proximal operator
		$$
		\pi_G(y;\tau) = \text{argmin}_{x}\frac{\|x-y\|_2^2}{2\tau}+G(x)
		$$
		<br>
		Primal-dual MS image segmentation:  
		$$\begin{align}\min_{u=(u_l)_{l=1}^k} \max_{p=(p_l)_{l=1}^k} &\left(\sum_{l=1}^k\langle \nabla u_l, p_l \rangle +\langle u_l, f_l \rangle \right) +\delta_U(u) - \delta_P(p) \end{align}$$
		So,
		<ul>
		<li> $K = \nabla$ (first order forward difference)
		<li> $K^* = -\text{div}$ (first order backward difference)
		<li> $F^*(p) = \delta_P(p)$ with $P=\left\{ p\in Y^k:\|p_l\|_\infty \le \frac{1}{2}\right\}$
		<li> $G(u) = \delta_U(u)$
		</ul>
		Proximal operaters $\pi_{F^*}$ and $\pi_G$ are pixel-wise projection onto convex sets $P$ and $U$
		$\Rightarrow$ Easy to implement on GPU
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<!-- <section>
	<h2>Mumford-Shah image segmentation</h2>
	<div id="left">
		Proximal operaters $\pi_{F^*}$ and $\pi_G$ are pixel-wise projection onto convex sets $P$ and $U$<br>
		$\Rightarrow$ Easy to implement on GPU
	</div>
	<div id="right"></div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section> -->

	<section>
	<h2>Segmentation of label tracked regions</h2>

		<p>For $K$ reference frames and $L$ iframes, let $f_{ij}(x)$ represent the optic flow error in using reference image $i$ to construct image $j$.  
		$$\begin{align}\min_{u} \max_{p} \sum_{l=1}^L\left(\sum_{k=1}^K\langle \nabla u_{kl}, p_{kl} \rangle +\langle u_{kl}, f_{kl} \rangle \right) +\delta_U(u) - \delta_P(p) \end{align}$$
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			1. Select very sparse set of reference frames (ref frames)<br>
			2. Select regular set of inter-frames (iframes)<br>
			<img src="assets/dend_d1_tile_c1_hl.png" width="90%"><br>
			<img src="assets/dend_d1_tile_c2_hl.png" width="90%"><br>
			3. Use optic flow+image segmentation to label regions mapping to reference frames<br>
			<img src="assets/mfsf_extend1.png" width="90%"><br>
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			4. Within each iframe block run MFSF for dense registration<br>
			<img src="assets/mfsf_extend2.png" width="90%"><br>
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			5. Associate each path from (4) with a ref frame using segmentation (3)<br>
			<img src="assets/mfsf_extend3.png" width="90%"><br>
		</p>
		<p class="fragment">
			Thus every tracked path is associated with a point in a reference frame
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
		Thus every tracked path is associated with a point in a reference frame
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Choosing ref frames</h2>
	<p>
		Want number of ref frames to balance global registration vs registration error <br><br>
		Add a group LASSO penalty for number of reference frames used:
		$$\begin{align}\min_{u} \sum_{l=1}^L\left(\sum_{k=1}^K \langle u_{kl}, f_{kl} \rangle \right) +\delta_U(u) - \delta_P(p) + \frac{\lambda_2}{2}\sum_{k=1}^K\left(\sum_{l=1}^L \|u_{kl}\|^2_2 \right)^{1/2} \end{align}$$
		The proximal operator $\pi_G$ now becomes:
		$$
		\pi_G(y;\tau) = \text{argmin}_{x}\frac{\|x-y\|_2^2}{2\tau}+\frac{\lambda_2}{2}\sum_{k=1}^K\left(\sum_{l=1}^L \|y_{kl}\|^2_2 \right)^{1/2}+\delta_U(y)
		$$
		Compute $\pi_G$ with ADMM


	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Extending with $K=2; L = 8$</h2>
	<video src="assets/mfsf_dm_combined.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="55%"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •
	   	We can see that this work fairly well for short sequences of video.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<!-- <section>
	<h2>Combining reference frames through temporal continuity of neuron tracking</h2>
	<video src="assets/mfsf_dm_combined.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •
	Allows for registration of neurons over videos of indefinite length
	    • <span style="color: green"></span>
	  </aside>
	</section> -->

	<section>
	<h2>Future work</h2>
	<ul>
		<li> Create larger hand annotated datasets for performance evaluation
		<li> Use temporal continuity between adjacent blocks to relate reference frames to one another
	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    This approach has utility as a general optic flow technique that can handle occlusions -- an important problem in computer vision.

	    We've seen how this method can be used to track Hydra, which we believe will
	    form an important component of determining the neural basis of its behavior.

	    • <span style="color: green"></span>
	  </aside>
	</section>


    <!--<script src="reveal.js/js/reveal.min.js"></script>-->
    <script src="reveal.js/js/reveal.js"></script>
    <script src="pdfjs/compatibility.js"></script>
    <script src="pdfjs/pdf.js"></script>

    <script src="lib/js/head.min.js"></script>
    <script>
      head.js(
        "lib/js/jquery.min.js",
        "lib/js/jquery.hotkeys.js",
        "lib/js/underscore.min.js",
        "lib/js/swfobject.js",
        "lib/js/dat.gui.js",
        "lib/js/EventEmitter.js",

        "lib/js/three.js",
        "lib/js/three/EffectComposer.js",
        "lib/js/three/RenderPass.js",
        "lib/js/three/BloomPass.js",
        "lib/js/three/ShaderPass.js",
        "lib/js/three/MaskPass.js",

        // three shaders
        "lib/js/three/shaders/CopyShader.js",
        "lib/js/three/shaders/BasicShader.js",
        "lib/js/three/shaders/DotScreenShader.js",
        "lib/js/three/shaders/UnpackDepthRGBAShader.js",
        "lib/js/three/shaders/HorizontalBlurShader.js",
        "lib/js/three/shaders/VerticalBlurShader.js",

        // js files needed for WebGL specific samples (excluding three js)
        "lib/js/J3DI.js",
        "lib/js/J3DIMath.js",
        "lib/js/webgl-utils.js",
        "lib/js/webgl-debug.js",

        // App specific js
        //"js/reveal.min.js",
        "js/stats_bootstrap.js",
        "js/samples.js",
        //"js/dat.gui.bootstrap.js",

		function() {
      Reveal.initialize({
        controls: false,
        progress: true,
        history: true,
        center: false,
        keyboard: true,
        touch: false,
        overview: true,
        mouseWheel: false,
        width: 960,
        height: 720,

        theme: false, // hardcoded with CSS import in <head>
        transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none
        transitionSpeed: 'default', // default/fast/slow

        math: {
          mathjax: 'mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        dependencies: [
          { src: 'reveal.js/lib/js/classList.js',
	    condition: function() { return !document.body.classList; }},
          { src: 'reveal.js/plugin/markdown/marked.js',
	    condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal.js/plugin/markdown/markdown.js',
	    condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true,
	    callback: function() { hljs.initHighlightingOnLoad (); }},
          { src: 'reveal.js/plugin/notes/notes.js', async: true,
	    condition: function() { return !!document.body.classList; }},
          { src: 'mymath.js', async: true },
	  { src: 'pdfimgs.js', async: true },
	  { src: 'slideautostart.js', async: true },
        ],
      });
    });
    </script>
  </body>
</html>
